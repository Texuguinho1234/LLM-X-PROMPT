{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Verificação de GPU e Configuração de Dispositivo\n",
        "Neste trecho inicial, estamos verificando se temos uma GPU disponível para acelerar o processamento.\n",
        "Caso uma GPU esteja disponível, o código utiliza o dispositivo `cuda`; caso contrário, o processamento será feito na CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# vendo se temos GPU disponível\n",
        "\n",
        "# !nvidia-smi <-- não vai funcionar se não tiver GPU!\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import transformers\n",
        "\n",
        "print(torch.cuda.is_available()) # retorna True se encontrou uma GPU para mandarmos jobs\n",
        "\n",
        "# dinamicamente identificando o dispositivo\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Carregando o modelo de linguagem TinyLlama\n",
        "Aqui utilizamos o pipeline da biblioteca `transformers` para carregar o modelo `TinyLlama-1.1B-Chat-v1.0`. O modelo é carregado no dispositivo previamente identificado (GPU ou CPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "llama = transformers.pipeline(\n",
        "    task='text-generation',\n",
        "    model='TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(llama)\n",
        "print(llama.model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Geração de Texto com o Modelo\n",
        "Neste passo, o modelo é utilizado para gerar texto com base no `prompt` dado. Definimos o número máximo de novos tokens para a resposta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"How are you?\"\n",
        "\n",
        "output = llama(prompt, max_new_tokens=25)\n",
        "\n",
        "\n",
        "print(output)\n",
        "print(output[0]['generated_text']) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Codificação do Prompt\n",
        "O prompt de entrada é convertido em IDs de tokens. Esses IDs são então enviados para o dispositivo e utilizados pelo modelo para gerar a sequência de saída."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_input_ids = llama.tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "print(prompt_input_ids)\n",
        "\n",
        "output = llama.model.generate(\n",
        "    prompt_input_ids, max_new_tokens=25,\n",
        "    return_dict_in_generate=True,\n",
        "    output_scores=True, \n",
        ")\n",
        "\n",
        "print(output.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Computando as Pontuações de Transição\n",
        "Após a geração de tokens, o código calcula as pontuações de transição para cada token gerado, utilizando os logits gerados pelo modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "transitions = llama.model.compute_transition_scores(\n",
        "    output.sequences, output.scores, normalize_logits=True\n",
        ")\n",
        "\n",
        "transitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Análise dos Tokens Gerados\n",
        "Aqui, o código exibe as pontuações dos tokens gerados após o prompt, ignorando o prompt original. Além disso, ele mostra a probabilidade associada a cada token gerado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tamanho_prompt = len(prompt_input_ids[0])\n",
        "print(tamanho_prompt)\n",
        "\n",
        "# nosso prompt faz parte da resposta gerada!\n",
        "print(output.sequences[0][:tamanho_prompt])\n",
        "\n",
        "# Pegando soh o que foi gerado\n",
        "generated_tokens = output.sequences[0][tamanho_prompt:]\n",
        "print(generated_tokens)\n",
        "\n",
        "print(' token id | score  | token str | prob % ')\n",
        "for (token, score) in zip(generated_tokens, transitions[0]):\n",
        "    if llama.tokenizer.decode(token) == '\\n':\n",
        "      continue\n",
        "\n",
        "    print(f\"| {token:8d} | {score.to('cpu').numpy():.3f} | {llama.tokenizer.decode(token):9s} | {np.exp(score.to('cpu').numpy()):.4f} |\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Medindo o Tempo de Execução\n",
        "Nesta célula, é medido o tempo que o modelo leva para gerar a saída com 100 novos tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "start = time.time()\n",
        "\n",
        "output = llama(prompt, max_new_tokens=100)\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "print(llama.model)\n",
        "print(f'Tempo executado: {end - start:.2f} ')\n",
        "print(output)\n",
        "print(output[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Gerando Respostas para Vários Prompts\n",
        "Esta célula faz uma interação com dois prompts diferentes e compara as respostas do modelo. Aqui, estamos testando a memória da conversa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lembrando da conversa\n",
        "\n",
        "prompt1 = \"What day is today?\"\n",
        "prompt2 = \"What day is tomorrow?\"\n",
        "\n",
        "output = llama(prompt1, max_new_tokens=10)\n",
        "print(output[0]['generated_text'])\n",
        "\n",
        "print(\"-\"*80)\n",
        "\n",
        "output = llama(prompt2, max_new_tokens=10)\n",
        "print(output[0]['generated_text'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Testando a Continuidade da Conversa\n",
        "O prompt de saída do primeiro comando é utilizado como entrada para o segundo comando, junto com um novo prompt, para ver como o modelo continua a conversa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt1 = \"What day is today?\"\n",
        "prompt2 = \"What day is tomorrow?\"\n",
        "\n",
        "output = llama(prompt1, max_new_tokens=10)\n",
        "print(output[0]['generated_text'])\n",
        "\n",
        "print(\"-\"*80)\n",
        "\n",
        "output = llama(output[0]['generated_text'] + prompt2, max_new_tokens=30)\n",
        "print(output[0]['generated_text'])"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}