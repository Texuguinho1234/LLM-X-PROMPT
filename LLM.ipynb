{"cells":[{"cell_type":"markdown","id":"fc282df4","metadata":{},"source":["# Documentação do Notebook - Geração de Texto com TinyLlama\n","\n","## Descrição\n","Este notebook utiliza a biblioteca `transformers` para executar uma tarefa de geração de texto usando o modelo de linguagem pré-treinado **TinyLlama-1.1B-Chat-v1.0**. O código foi configurado para rodar em GPU (ou CPU, se especificado).\n","\n","### Requisitos\n","- **Python 3.x**\n","- **Transformers** (`pip install transformers`)\n","- Acesso a uma GPU compatível com CUDA para melhor desempenho (opcional, mas recomendado)\n"]},{"cell_type":"markdown","id":"513a5740","metadata":{},"source":["## Etapas do Notebook\n","\n","### 1. Verificar o Uso da GPU\n","```python\n","# Vendo GPU:\n","!nvidia-smi\n","```\n","Essa célula utiliza o comando `nvidia-smi` para exibir informações sobre a GPU disponível na máquina (se houver). Se estiver utilizando uma GPU, é recomendado verificar o estado da GPU antes de rodar o modelo.\n"]},{"cell_type":"markdown","id":"cebc5d49","metadata":{},"source":["### 2. Carregar o Modelo de Geração de Texto\n","```python\n","import transformers\n","\n","bumbumblee = transformers.pipeline(\n","    task=\"text-generation\",\n","    model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n","    device=\"cuda\"  # Pode ser 'cpu' ou 'cuda' dependendo da disponibilidade\n",")\n","```\n","Nesta célula, o modelo **TinyLlama-1.1B-Chat-v1.0** é carregado através da função `pipeline` da biblioteca `transformers`. A tarefa específica é `text-generation`, e o código está configurado para rodar em uma GPU (`device=\"cuda\"`), caso disponível.\n"]},{"cell_type":"markdown","id":"b0037cce","metadata":{},"source":["### 3. Execução do Modelo\n","```python\n","bumbumblee\n","```\n","Aqui, o pipeline carregado anteriormente é executado. Presume-se que nas células seguintes sejam passados textos ou prompts para a geração de respostas, embora esta célula específica apenas exiba o objeto `bumbumblee`.\n"]},{"cell_type":"markdown","id":"f6e00e6a","metadata":{},"source":["## Observações\n","- O modelo **TinyLlama-1.1B-Chat-v1.0** é um modelo otimizado para conversação e tarefas de geração de texto, sendo uma versão mais leve e eficiente de grandes modelos de linguagem.\n","- A utilização de GPU melhora significativamente a performance, especialmente em tarefas mais intensas como a geração de texto.\n"]},{"cell_type":"markdown","id":"5292f001","metadata":{},"source":["## Próximos Passos\n","- Fornecer prompts de entrada para o modelo e explorar as saídas geradas.\n","- Avaliar a performance do modelo tanto em CPU quanto em GPU.\n","- Ajustar os parâmetros do pipeline, como a quantidade de texto gerado, temperatura, entre outros.\n"]},{"cell_type":"markdown","id":"5fc140a9","metadata":{},"source":["## Referências\n","- [Transformers Documentation](https://huggingface.co/docs/transformers)\n","- [TinyLlama Model](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0)\n"]},{"cell_type":"code","execution_count":null,"id":"85b23850","metadata":{},"outputs":[],"source":["# Vendo GPU:\n","!nvidia-smi"]},{"cell_type":"code","execution_count":null,"id":"b8f0c994","metadata":{},"outputs":[],"source":["import transformers\n","\n","bumbumblee = transformers.pipeline(\n","    task=\"text-generation\",\n","    model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n","    device=\"cuda\"  # Pode ser 'cpu' ou 'cuda' dependendo da disponibilidade\n",")"]},{"cell_type":"code","execution_count":null,"id":"ff4dc8ad","metadata":{},"outputs":[],"source":["bumbumblee"]},{"cell_type":"code","execution_count":null,"id":"f58c46d4","metadata":{},"outputs":[],"source":["bumbumblee.model"]},{"cell_type":"code","execution_count":null,"id":"553ebf79","metadata":{},"outputs":[],"source":["def analisa_e_processa_prompt(modelo, prompt):\n","  print(\"=\"*80)\n","\n","  # codificando\n","  tokens = modelo.tokenizer.encode(prompt, return_tensors=\"pt\")\n","\n","  print(f\"- prompt original: {prompt}\")\n","  print(f\"- token ids:{tokens}\")\n","  print(f\"- # de tokens:{len(tokens)}\")\n","\n","  # decodificando\n","  decoded = modelo.tokenizer.convert_ids_to_tokens(tokens[0])\n","  print(f\"- tokens decodificados: {decoded}\")\n","\n","  first_tokens = modelo.tokenizer.convert_ids_to_tokens(range(5))\n","  print(f\"- primeiros tokens: {first_tokens}\")\n","\n","  # Prompt pro LLM. Temperatura controla o quao \"criativo\" é\n","  output = modelo(prompt, do_sample=True, temperature=0.7)\n","  print(f\"- output do modelo: {output}\")\n","  print(f\"- texto gerado efetivamente: {output[0]['generated_text']}\")\n","\n","  print(\"=\"*80)\n","  return\n","\n","analisa_e_processa_prompt(bumbumblee, \"Tell a history bro\")"]}],"metadata":{"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":5}
